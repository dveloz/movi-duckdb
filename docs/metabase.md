# Metabase
## About
This repository contains a Dockerfile that when built can be used a Metabase 
deployment.

## How it works
The Dockerfile builds an image which consists of two main layers.

### 1. Python 3.10
With this layer we clone the [Movicar DuckDB](https://github.com/dveloz/movi-duckdb) 
repository. 

Afterwards we run the `duck_setup.py` script which generates a duckdb file with 
all the configurated views, which points at Movicar's DataLake S3 Bucket.

The file generated by this script is what will allow us to run queries from our
S3 data in a simpler way 

**Example:**
```
SELECT * FROM facts.usuarios;

## instead of

SELECT * FROM read_parquet('s3://movi-data-lake/facts/usuarios/');
```

In this past example both of our queries are reading from the same S3 files but 
the first query format is much more simpler.  
Additionally, the generated files will allow us to see the available schemas and 
tables without the need to explore the S3 resources.

Note:
In case that you add new tables to the S3 DataLake, this script will need to be 
run again. Because it scrapes the existing files.

### 2. OpenJDK  
This layer contains the necessary files to run a Metabase instance as well as 
the necessary plugins for Metabase to be able to read a DuckDB File.


## Building the image
When building the image make sure to include the correct AWS credentials as 
build arguments.

`docker build --build-arg AWS_ACCESS_KEY_ID=<MY_AWS_ACCESS_KEY> --build-arg AWS_SECRET_ACCESS_KEY=<MY_AWS_SECRET_ACCESS_KEY> . --tag metaduck:latest`

## Running the image
Finally, just run the image. Make sure to provide suficcient memory.

`docker run -m 8GB metaduck`

## Setting up Metabase
Once deployed and running, Metabase should be able to add a DuckDB data source. 
Be sure to select the path to the previously generated `.duckdb` file. Located in 
`~\movicar-duckdb\analytics-prod.duckdb`. Read Only mode recommended.

After connecting to the DuckDB file, you should now be able to see the existing 
schemas and tables.  

## Providing permissions to our Metabase
Unfortunately, if we try to query them we will receive 40X responses from S3.
In order to fix this issue, just open a Query Window and run the following 
commands.

```
SET s3_access_key_id='<MY_AWS_ACCESS_KEY>';
SET s3_secret_access_key='<MY_AWS_SECRET_ACCESS_KEY>';
SET s3_region='<us-west-1>'
```

For the time being there is no workaround for this issue, although recently, 
DuckDB version 0.9 has been released. With this new version we can preppend to 
any query, the `CALL load_aws_credentials();` command, which will help us to automatically 
load our credentials by using our AWS Credential Provider Chain. 

As soon as the 
[Metabase Plugin](https://github.com/AlexR2D2/metabase_duckdb_driver/releases), 
releases a version that supports DuckDB 0.9.1, we can stop using this messy 
workaround.

